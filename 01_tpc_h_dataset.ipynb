{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb2749b9-0ac6-40f6-a136-fd3e37debaba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Answer to question \n",
    "# for the TPC_H_Dataset part\n",
    "#-------------\n",
    "#Author: AdrianJ\n",
    "#V1.0 Created(2023-09-01)\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26345df6-7ec2-4634-862c-50c7e47edaf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746f637c-c09a-4bdb-8cf0-fa448fed9556",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **Question #1**: Joins in Core Spark\n",
    "Pick any two datasets and join them using Spark's API. Feel free to pick any two datasets. For example: `PART` and `PARTSUPP`. The goal of this exercise is not to derive anything meaningful out of this data but to demonstrate how to use Spark to join two datasets. For this problem, you're **NOT allowed to use SparkSQL**. You can only use RDD API. You can use either Python or Scala to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466f7667-d35c-4981-acd8-f2ab8df5667e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:The PARTSUPP object enriched with PART object\n Example of one row [('24', (('25', '5180', '905.41', 'heodolites above the ironic requests poach fluffily carefully unusual pinto beans. even packages acc', ''), ('seashell coral metallic midnight floral', 'Manufacturer#5', 'Brand#52', 'MEDIUM PLATED STEEL', '20', 'MED CASE', '924.02', ' final the', '')))]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method RDD.unpersist of PythonRDD[791] at RDD at PythonRDD.scala:58>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For this join I will be enriching PARTSUPP with PART table.\n",
    "#function to separete columns \n",
    "\n",
    "def parser(line):\n",
    "    \"\"\"\n",
    "    Parse the rdd as expression with a separetor\n",
    "    have the ability to select the number\n",
    "    @line: row from text file\n",
    "\n",
    "    \"\"\"\n",
    "    r = line.split('|')\n",
    "    \n",
    "    return (r[0], tuple(r[n] for n in range(len(r)) if n != 0))\n",
    "\n",
    "#read part(P_)\n",
    "p_file_path = '/databricks-datasets/tpch/data-001/part/'\n",
    "p_rdd = (\n",
    "    sc.textFile(p_file_path, 2)\n",
    ")\n",
    "\n",
    "p_rdd = p_rdd.map(lambda line : parser(line))\n",
    "p_rdd = p_rdd.cache()\n",
    "\n",
    "#read partsupp(ps_)\n",
    "ps_file_path = '/databricks-datasets/tpch/data-001/partsupp/'\n",
    "ps_rdd = (\n",
    "    sc.textFile(ps_file_path, 2)\n",
    ")\n",
    "\n",
    "ps_rdd = ps_rdd.map(lambda line : parser(line))\n",
    "\n",
    "#join p and ps table\n",
    "join = ps_rdd.leftOuterJoin(p_rdd)\n",
    "\n",
    "print(\"INFO:The PARTSUPP object enriched with PART object\")\n",
    "print(f\" Example of one row {join.take(1)}\")\n",
    "ps_rdd.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8752994a-ea49-46d0-9b9b-5a18fda82014",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **Question #2**: Joins With Spark SQL\n",
    "Pick any two datasets and join them using SparkSQL API. Feel free to pick any two datasets. For example, PART and PARTSUPP. The goal of this exercise is not to derive anything meaningful out of this data but to demonstrate how to use Spark to join two datasets. For this problem, you're **NOT allowed to use the RDD API**. You can only use SparkSQL API. You can use either Python or Scala to solve this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ce4c93-11c6-45b5-879c-fa9a411761b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:The PARTSUPP object enriched with part name from PART on PARTKEY\n List of columns from PARTSUPP ['PARTKEY', 'SUPPKEY', 'AVAILQTY', 'SUPPLYCOST', 'COMMENT', 'NAME']\n Example of one row\n+-------+-------+--------+----------+--------------------+--------------------+\n|PARTKEY|SUPPKEY|AVAILQTY|SUPPLYCOST|             COMMENT|                NAME|\n+-------+-------+--------+----------+--------------------+--------------------+\n|      1|      2|    3325|    771.64|, even theodolite...|goldenrod lavende...|\n+-------+-------+--------+----------+--------------------+--------------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#For this join I will be enriching PARTSUPP with the part name from the PART table.\n",
    "#read part(P_)\n",
    "p_schema = StructType([\n",
    "    StructField(\"PARTKEY\",IntegerType(),False),\n",
    "    StructField(\"NAME\",StringType(),True),\n",
    "    StructField(\"MFGR\",StringType(),True),\n",
    "    StructField(\"BRAND\",StringType(),True),\n",
    "    StructField(\"TYPE\",StringType(),True),\n",
    "    StructField(\"SIZE\",StringType(),True),\n",
    "    StructField(\"CONTAINER\",StringType(),True),\n",
    "    StructField(\"RETAILPRICE\",StringType(),True),\n",
    "    StructField(\"COMMENT\",StringType(),True)\n",
    "])\n",
    "\n",
    "p_file_path = '/databricks-datasets/tpch/data-001/part/'\n",
    "p_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(sep=\"|\", header=\"false\", inferSchema=\"false\")\n",
    "    .schema(p_schema)\n",
    "    .load(p_file_path)\n",
    "    .select(\n",
    "        F.col(\"PARTKEY\"),\n",
    "        F.col(\"NAME\")\n",
    "    )\n",
    ")\n",
    "\n",
    "#read partsupp(ps_)\n",
    "ps_schema = StructType([\n",
    "    StructField(\"PARTKEY\",IntegerType(),False),\n",
    "    StructField(\"SUPPKEY\",StringType(),False),\n",
    "    StructField(\"AVAILQTY\",StringType(),True),\n",
    "    StructField(\"SUPPLYCOST\",StringType(),True),\n",
    "    StructField(\"COMMENT\",StringType(),True)\n",
    "])\n",
    "\n",
    "ps_file_path = '/databricks-datasets/tpch/data-001/partsupp/'\n",
    "ps_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(sep=\"|\", header=\"false\", inferSchema=\"false\")\n",
    "    .schema(ps_schema)\n",
    "    .load(ps_file_path)\n",
    ")\n",
    "\n",
    "#enrich PARTSUPP with part name from PART\n",
    "ps_df = ps_df.join(\n",
    "    F.broadcast(p_df),\n",
    "    on=[\"PARTKEY\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"INFO:The PARTSUPP object enriched with part name from PART on PARTKEY\")\n",
    "print(f\" List of columns from PARTSUPP {ps_df.columns}\")\n",
    "print(f\" Example of one row\")\n",
    "ps_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20efd8ec-a5ea-42d9-b4ee-8630e368d4fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **Question #3**: Alternate Data Formats\n",
    "The given dataset above is in raw text storage format. What other data storage format can you suggest optimizing the performance of our Spark workload if we were to frequently scan and read this dataset. Please come up with a code example and explain why you decide to go with this approach. Please note that there's no completely correct answer here. We're interested to hear your thoughts and see the implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514612b6-1570-4963-b484-b7c7923a77b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Successfully created delta table -> de_coding_challenge.PART\nINFO: Successfully created delta table -> de_coding_challenge.PARTSUPP\n"
     ]
    }
   ],
   "source": [
    "# The format I would choose to optimize the would be any parquet format file. \n",
    "# In my experience Delta lake format is a very good format due to the multiple compatibility that \n",
    "# it has with a platform like databricks. For example: ACID Transactions, Schema Enforcement and Evolution and 'time travel'\n",
    "# NB! Databricks create delta tables by default. However, for sake of the exercise I'm declaring the format\n",
    "\n",
    "#Example code below:\n",
    "def create_delta_table(df: SparkDataFrame, schema_name: str, mode: str, table_name: str):\n",
    "    \"\"\"\n",
    "    This function takes a SparkDataFrame as an input and creates a managed delta table\n",
    "    Then checks if table was succefully created otherwise raise error\n",
    "    \"\"\"\n",
    "    spark.sql(f\"\"\"CREATE SCHEMA IF NOT EXISTS de_coding_challenge\"\"\")\n",
    "\n",
    "    (df.write.mode(mode).format(\"delta\").saveAsTable(f\"{schema_name}.{table_name}\"))\n",
    "    #check if table was created successfully\n",
    "    table_exists = spark.catalog.tableExists(f\"{schema_name}.{table_name}\")\n",
    "    if table_exists:\n",
    "        print(f\"INFO: Successfully created delta table -> {schema_name}.{table_name}\")\n",
    "    else:\n",
    "        e = f\"ERROR: Failed to create table -> {schema_name}.{table_name}\"\n",
    "        raise Exception(e)\n",
    "\n",
    "#schema variables\n",
    "schema_name = \"de_coding_challenge\"\n",
    "mode = \"overwrite\"\n",
    "\n",
    "#create PART table from question 2\n",
    "create_delta_table(p_df, schema_name, mode, \"PART\")\n",
    "\n",
    "#Create PARTSUPP delta table from question 2\n",
    "create_delta_table(ps_df, schema_name, mode, \"PARTSUPP\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_tpc_h_dataset",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
