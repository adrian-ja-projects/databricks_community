{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a21f7dc8-d1a0-469e-9e59-1ad991d6a6f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Answer to question \n",
    "# csv parsing\n",
    "#-------------\n",
    "#Author: AdrianJ\n",
    "#V1.0 Created(2023-09-01)\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9679e0-ac81-41cf-ad97-b932d0a746a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76bde41e-0eda-4abb-b983-c1ef9e677302",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### CSV Parsing\n",
    "The following examples involve working with simple CSV data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59fb1a5-a67d-4563-9dd0-548d07f570fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **Question #1**: CSV Header Rows\n",
    "Given the simple RDD `full_csv` below, write the most efficient Spark job you can to remove the header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2458ee3e-036b-4e42-846e-435a949c5fe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Example of the csv file without the first row\n",
       "+--------------+\n",
       "         value|\n",
       "+--------------+\n",
       "  1, ABC, Foo1|\n",
       " 2, ABCD, Foo2|\n",
       "3, ABCDE, Foo3|\n",
       "+--------------+\n",
       "only showing top 3 rows\n",
       "\n",
       "()\n",
       "full_csv: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[754] at parallelize at command-2172150280468801:1\n",
       "no_header_csv: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[757] at map at command-2172150280468801:16\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Example of the csv file without the first row\n+--------------+\n|         value|\n+--------------+\n|  1, ABC, Foo1|\n| 2, ABCD, Foo2|\n|3, ABCDE, Foo3|\n+--------------+\nonly showing top 3 rows\n\n()\nfull_csv: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[754] at parallelize at command-2172150280468801:1\nno_header_csv: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[757] at map at command-2172150280468801:16\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val full_csv = sc.parallelize(Array(\n",
    "  \"col_1, col_2, col_3\",\n",
    "  \"1, ABC, Foo1\",\n",
    "  \"2, ABCD, Foo2\",\n",
    "  \"3, ABCDE, Foo3\",\n",
    "  \"4, ABCDEF, Foo4\",\n",
    "  \"5, DEF, Foo5\",\n",
    "  \"6, DEFGHI, Foo6\",\n",
    "  \"7, GHI, Foo7\",\n",
    "  \"8, GHIJKL, Foo8\",\n",
    "  \"9, JKLMNO, Foo9\",\n",
    "  \"10, MNO, Foo10\",\n",
    "))\n",
    "\n",
    "//remove by index to avoid accidentally removing rows that contain the same value than the header\n",
    "val no_header_csv = full_csv.zipWithIndex().filter { case (_, index) => index > 0 }.map { case (line, _) => line }\n",
    "\n",
    "println(\"Example of the csv file without the first row\")\n",
    "println(no_header_csv.toDF().show(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "613abb52-3336-4777-a9c0-02120f42c21c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Question #2 DataFrame UDFs and DataFrame SparkSQL Functions\n",
    "\n",
    "Below we've created a small DataFrame. You should use DataFrame API functions and UDFs to accomplish two tasks.\n",
    "\n",
    "1. You need to parse the State and city into two different columns.\n",
    "2. You need to get the number of days in between the start and end dates. You need to do this two ways.\n",
    "  - Firstly, you should use SparkSQL functions to get this date difference.\n",
    "  - Secondly, you should write a udf that gets the number of days between the end date and the start date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05e720c-f324-4398-8748-db300b40f30c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 272 bytes.\nroot\n |-- id: string (nullable = true)\n |-- end_date: string (nullable = true)\n |-- start_date: string (nullable = true)\n |-- location: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Build an example DataFrame dataset to work with. \n",
    "dbutils.fs.rm(\"/tmp/dataframe_sample.csv\", True)\n",
    "dbutils.fs.put(\"/tmp/dataframe_sample.csv\", \"\"\"id|end_date|start_date|location\n",
    "1|2015-10-14 00:00:00|2015-09-14 00:00:00|CA-SF\n",
    "2|2015-10-15 01:00:20|2015-08-14 00:00:00|CA-SD\n",
    "3|2015-10-16 02:30:00|2015-01-14 00:00:00|NY-NY\n",
    "4|2015-10-17 03:00:20|2015-02-14 00:00:00|NY-NY\n",
    "5|2015-10-18 04:30:00|2014-04-14 00:00:00|CA-SD\n",
    "\"\"\", True)\n",
    "\n",
    "formatPackage = \"csv\" if sc.version > '1.6' else \"com.databricks.spark.csv\"\n",
    "df = sqlContext.read.format(formatPackage).options(\n",
    "  header='true', \n",
    "  delimiter = '|',\n",
    ").load(\"/tmp/dataframe_sample.csv\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b80da3-55e9-4a95-8d42-aa40d47ae8f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Showing the state and city parsed into two columns\n+---+-------------------+-------------------+----+-----+\n| id|           end_date|         start_date|city|State|\n+---+-------------------+-------------------+----+-----+\n|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|  CA|   SF|\n|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|  CA|   SD|\n|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|  NY|   NY|\n|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|  NY|   NY|\n|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|  CA|   SD|\n+---+-------------------+-------------------+----+-----+\n\nINFO: Showing the number of days between to dates using and UDF\n+---+-------------------+-------------------+----+-----+------------+\n| id|           end_date|         start_date|city|State|days_between|\n+---+-------------------+-------------------+----+-----+------------+\n|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|  CA|   SF|          30|\n|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|  CA|   SD|          62|\n|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|  NY|   NY|         275|\n|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|  NY|   NY|         245|\n|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|  CA|   SD|         552|\n+---+-------------------+-------------------+----+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#parse the city and state in two different columns\n",
    "df = (\n",
    "    df\n",
    "    .select(\n",
    "        F.col(\"id\"),\n",
    "        F.col(\"end_date\"),\n",
    "        F.col(\"start_date\"),\n",
    "        F.split(\"location\", \"-\")[0].alias(\"city\"),\n",
    "        F.split(\"location\", \"-\")[1].alias(\"State\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"INFO: Showing the state and city parsed into two columns\")\n",
    "df.show()\n",
    "\n",
    "#Create a udf that return number of days between the end and start date\n",
    "\n",
    "def calculate_days_between(start_date: str, end_date:str):\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S')\n",
    "    diff = end_date - start_date\n",
    "    return diff.days\n",
    "\n",
    "#register udf\n",
    "calculate_days_between_udf = F.udf(calculate_days_between)\n",
    "\n",
    "print(\"INFO: Showing the number of days between to dates using and UDF\")\n",
    "df.withColumn(\"days_between\", calculate_days_between_udf(F.col(\"start_date\"), F.col(\"end_date\"))).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_csv_parsing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
